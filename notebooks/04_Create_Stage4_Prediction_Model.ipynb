{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5692c5b5-d5f5-4cab-a7a4-4cc34d7c8230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For imbalance data\n",
    "%pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4731cc6a-8667-43b0-9832-938814019560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ffa941-020f-4902-b18e-9730b1cd23cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# COMMAND ----------\n",
    "df_spark = spark.table(\"cross_sell_insurance.01_feature_staging.stage2_clean_feature_table\")\n",
    "print(\"Spark schema:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "df = df_spark.toPandas()\n",
    "print(\"Pandas shape:\", df.shape)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfaa4f3-6c9d-4723-9f7a-2eec513ea6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Define target, cek imbalance, dan train–test split\n",
    "\n",
    "# COMMAND ----------\n",
    "TARGET_COL = \"is_target_customer\"\n",
    "\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(\"Class distribution (count & %):\")\n",
    "counts = y.value_counts()\n",
    "pct = y.value_counts(normalize=True) * 100\n",
    "display(pd.DataFrame({\"count\": counts, \"percentage\": pct.round(4)}))\n",
    "\n",
    "total = len(y)\n",
    "pos = (y == 1).sum()\n",
    "neg = (y == 0).sum()\n",
    "ratio = neg / pos if pos > 0 else np.inf\n",
    "\n",
    "print(f\"\\nTotal samples : {total}\")\n",
    "print(f\"Class 0 (non-target): {neg} ({neg/total*100:.4f}%)\")\n",
    "print(f\"Class 1 (target)    : {pos} ({pos/total*100:.4f}%)\")\n",
    "print(f\"Imbalance ratio (neg:pos) ≈ {ratio:.2f} : 1\")\n",
    "\n",
    "# Train-test split (stratified karena imbalance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain shape:\", X_train.shape)\n",
    "print(\"Test shape :\", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa2fe26-b769-454f-8595-2443e5c491cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Tentukan kolom kategorikal & numerik\n",
    "# COMMAND ----------\n",
    "# Numeric: kolom dengan dtype number / bool\n",
    "numeric_cols = X_train.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# Categorical: kolom object / category\n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "print(\"Numeric cols (first 20):\", numeric_cols[:20])\n",
    "print(\"Total numeric cols:\", len(numeric_cols))\n",
    "print(\"Categorical cols:\", categorical_cols)\n",
    "print(\"Total categorical cols:\", len(categorical_cols))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bdb4be5-5d2a-47ff-a9a4-97fa29bd8878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Preprocessing pipeline (Impute → Log (numeric) → Scale → OHE)\n",
    "\n",
    "# COMMAND ----------\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Numeric: isi missing dengan 0, lalu scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "# Categorical: isi missing dengan '0' (string), lalu OneHotEncode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"0\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adffd4b0-ac07-44af-ad54-e841c70c54ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Logistic Regression pipeline + cross validation (tanpa undersampling)\n",
    "\n",
    "# COMMAND ----------\n",
    "# Kita pakai class_weight='balanced' untuk handle extreme imbalance\n",
    "logreg_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=800,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        solver=\"lbfgs\",\n",
    "    )),\n",
    "])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ROC-AUC CV\n",
    "roc_scores = cross_val_score(\n",
    "    logreg_pipeline,\n",
    "    X_train, y_train,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# PR-AUC (Average Precision) CV\n",
    "pr_scores = cross_val_score(\n",
    "    logreg_pipeline,\n",
    "    X_train, y_train,\n",
    "    scoring=\"average_precision\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (class_weight='balanced')\")\n",
    "print(f\"ROC-AUC CV mean ± std: {roc_scores.mean():.4f} ± {roc_scores.std():.4f}\")\n",
    "print(f\"PR-AUC  CV mean ± std: {pr_scores.mean():.4f} ± {pr_scores.std():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c0950a-932d-40ce-a776-55951f8d9081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Train Logistic Regression final di full train & evaluasi di test\n",
    "# COMMAND ----------\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_proba_log = logreg_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred_log_default = (y_proba_log >= 0.5).astype(int)  # threshold default\n",
    "\n",
    "print(\"=== Logistic Regression – Test Evaluation (threshold=0.5) ===\")\n",
    "print(\"ROC-AUC :\", roc_auc_score(y_test, y_proba_log))\n",
    "print(\"PR-AUC  :\", average_precision_score(y_test, y_proba_log))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_log_default, digits=4))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_log_default))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c85c14e-4507-4df4-85c3-7dba6891da7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Decision Tree pipeline + CV (juga tanpa undersampling)\n",
    "# COMMAND ----------\n",
    "tree_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", DecisionTreeClassifier(\n",
    "        max_depth=5,\n",
    "        min_samples_leaf=50,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "tree_roc_scores = cross_val_score(\n",
    "    tree_pipeline,\n",
    "    X_train, y_train,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "tree_pr_scores = cross_val_score(\n",
    "    tree_pipeline,\n",
    "    X_train, y_train,\n",
    "    scoring=\"average_precision\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"DECISION TREE (class_weight='balanced')\")\n",
    "print(f\"ROC-AUC CV mean ± std: {tree_roc_scores.mean():.4f} ± {tree_roc_scores.std():.4f}\")\n",
    "print(f\"PR-AUC  CV mean ± std: {tree_pr_scores.mean():.4f} ± {tree_pr_scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d862c050-7fdd-4938-8488-f20657407126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. Train Decision Tree final & evaluasi di test\n",
    "# COMMAND ----------\n",
    "tree_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_proba_tree = tree_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred_tree_default = (y_proba_tree >= 0.5).astype(int)\n",
    "\n",
    "print(\"=== Decision Tree – Test Evaluation (threshold=0.5) ===\")\n",
    "print(\"ROC-AUC :\", roc_auc_score(y_test, y_proba_tree))\n",
    "print(\"PR-AUC  :\", average_precision_score(y_test, y_proba_tree))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_tree_default, digits=4))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_tree_default))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad6cc512-6713-44ef-83a6-892854cee9ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 9. Lihat feature importance Logistic Regression\n",
    "# COMMAND ----------\n",
    "# Ambil nama feature setelah preprocess\n",
    "num_feature_names = numeric_cols\n",
    "cat_feature_names = list(\n",
    "    logreg_pipeline.named_steps[\"preprocess\"]\n",
    "    .named_transformers_[\"cat\"]\n",
    "    .named_steps[\"onehot\"]\n",
    "    .get_feature_names_out(categorical_cols)\n",
    ")\n",
    "\n",
    "all_feature_names = np.concatenate([num_feature_names, cat_feature_names])\n",
    "\n",
    "coefs = logreg_pipeline.named_steps[\"clf\"].coef_[0]\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": all_feature_names,\n",
    "    \"coefficient\": coefs,\n",
    "}).sort_values(\"coefficient\", ascending=False)\n",
    "\n",
    "print(\"Top positive features (mendorong ke class 1):\")\n",
    "display(coef_df.head(20))\n",
    "\n",
    "print(\"Top negative features (mendorong ke class 0):\")\n",
    "display(coef_df.tail(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd97bca3-d046-4197-ae34-82f607f40711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ambil feature importance dari trained decision tree\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Ambil feature names setelah preprocessing\n",
    "num_feature_names = numeric_cols\n",
    "cat_feature_names = list(\n",
    "    tree_pipeline.named_steps[\"preprocess\"]\n",
    "    .named_transformers_[\"cat\"]\n",
    "    .named_steps[\"onehot\"]\n",
    "    .get_feature_names_out(categorical_cols)\n",
    ")\n",
    "\n",
    "all_feature_names = np.concatenate([num_feature_names, cat_feature_names])\n",
    "\n",
    "# Ambil feature importance dari tree\n",
    "tree_clf = tree_pipeline.named_steps[\"clf\"]\n",
    "importances = tree_clf.feature_importances_\n",
    "\n",
    "# Buat DataFrame\n",
    "tree_fi_df = pd.DataFrame({\n",
    "    \"feature\": all_feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "display(tree_fi_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854e6b2c-dc11-4f84-9282-4a4f602d5d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_n = 20\n",
    "top_fi = tree_fi_df.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top_fi[\"feature\"][::-1], top_fi[\"importance\"][::-1])\n",
    "plt.title(\"Top 20 Feature Importances (Decision Tree)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93dd2f5-3f90-45d9-ae09-74ed3e14f49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "tree_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15d16bb-1763-40b9-89c3-936459e432d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "y_proba_log = logreg_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_proba_tree = tree_pipeline.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdfd89c8-22b5-4d30-baaa-47a4456a7659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "metrics[\"logistic\"] = {\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_proba_log),\n",
    "    \"pr_auc\": average_precision_score(y_test, y_proba_log)\n",
    "}\n",
    "\n",
    "metrics[\"tree\"] = {\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_proba_tree),\n",
    "    \"pr_auc\": average_precision_score(y_test, y_proba_tree)\n",
    "}\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f2793a-dade-4bb3-b5c8-98bc4d311cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "best_model_name = max(metrics, key=lambda m: metrics[m][\"pr_auc\"])\n",
    "best_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d643f643-6d25-427b-90b5-a13950c705ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "if best_model_name == \"logistic\":\n",
    "    best_model = logreg_pipeline\n",
    "else:\n",
    "    best_model = tree_pipeline\n",
    "\n",
    "best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17d8b348-4670-4a66-a032-8c8bbe1a3c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e2d369-0662-42dd-88c7-7c60fc81d594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load full table untuk scoring\n",
    "df_spark_full = spark.table(\"cross_sell_insurance.01_feature_staging.stage2_clean_feature_table\")\n",
    "df_full = df_spark_full.toPandas()\n",
    "\n",
    "TARGET_COL = \"is_target_customer\"\n",
    "\n",
    "# X_all = semua fitur tanpa target\n",
    "X_all = df_full.drop(columns=[TARGET_COL])\n",
    "\n",
    "# Cari kolom ID utama (sesuaikan kalau kamu tahu pasti namanya)\n",
    "id_col = None\n",
    "for cand in [\"client_id\", \"customer_id\", \"accountid\", \"customer_number\"]:\n",
    "    if cand in df_full.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "\n",
    "# Kalau tidak ketemu, pakai index sebagai ID\n",
    "if id_col is None:\n",
    "    df_full[\"row_id\"] = np.arange(len(df_full))\n",
    "    id_col = \"row_id\"\n",
    "\n",
    "print(\"ID column used:\", id_col)\n",
    "\n",
    "# Prediksi probabilitas beli Bebas Aksi\n",
    "proba_all = best_model.predict_proba(X_all)[:, 1]\n",
    "\n",
    "# Threshold (bisa kamu ubah, misal 0.2)\n",
    "threshold = 0.2\n",
    "flag_all = (proba_all >= threshold).astype(int)\n",
    "\n",
    "# Buat DataFrame hasil scoring\n",
    "scored_df = pd.DataFrame({\n",
    "    id_col: df_full[id_col],\n",
    "    \"bebas_aksi_score\": proba_all,\n",
    "    \"bebas_aksi_flag\": flag_all\n",
    "})\n",
    "\n",
    "scored_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b24923b-8522-4842-8573-66d4c7b7ecb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simpan hasil scoring ke Spark table baru\n",
    "\n",
    "# COMMAND ----------\n",
    "scored_spark = spark.createDataFrame(scored_df)\n",
    "\n",
    "scored_spark.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"cross_sell_insurance.01_feature_staging.stage3_bebas_aksi_scored\"\n",
    ")\n",
    "\n",
    "# Cek hasil di Spark\n",
    "spark.table(\"cross_sell_insurance.01_feature_staging.stage3_bebas_aksi_scored\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9481083f-4596-434d-b2e9-84310523df3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.set_experiment(\"/Users/u2600038142@gmail.com/bebas_aksi_model\")   # opsional\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.log_model(best_model, \"bebas_aksi_model\")\n",
    "    mlflow.log_metric(\"roc_auc\", metrics[best_model_name][\"roc_auc\"])\n",
    "    mlflow.log_metric(\"pr_auc\", metrics[best_model_name][\"pr_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff3369aa-732a-4307-8b45-7c361b85104d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Create_Stage4_Prediction_Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
