{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11727fbd-9d73-4ba1-898a-f618f9af099b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_model.py\n",
    "\n",
    "Fokus:\n",
    "- Load data dari Spark\n",
    "- Train 2 model (LogReg & Decision Tree)\n",
    "- Pilih best model berdasarkan PR-AUC test\n",
    "- Log & register best model ke MLflow (minimal setup, sama seperti notebook yang sudah jalan)\n",
    "\n",
    "Tidak ada:\n",
    "- batch scoring\n",
    "- set_experiment / set_tracking_uri\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Logging\n",
    "# ==============================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"train_model\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Defaults (sesuai notebook-mu)\n",
    "# ==============================\n",
    "DEFAULT_FEATURE_TABLE = \"cross_sell_insurance.01_feature_staging.stage2_clean_feature_table\"\n",
    "DEFAULT_TARGET_COL = \"is_target_customer\"\n",
    "DEFAULT_REGISTERED_MODEL_NAME = \"ml_model.bebas_aksi_classifier_proba\"  # UC: catalog.schema.name\n",
    "\n",
    "\n",
    "def get_env_or_default(env_name: str, default: str) -> str:\n",
    "    \"\"\"Baca env var, kalau 'None'/'none' dianggap kosong dan pakai default.\"\"\"\n",
    "    val = os.getenv(env_name)\n",
    "    if val is None:\n",
    "        return default\n",
    "    if val.strip().lower() == \"none\":\n",
    "        return default\n",
    "    return val\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Argparse (optional, dengan default)\n",
    "# ==============================\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Train Cross-Sell Insurance Propensity Model\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--feature-table\",\n",
    "        required=False,\n",
    "        default=get_env_or_default(\"FEATURE_TABLE\", DEFAULT_FEATURE_TABLE),\n",
    "        help=\"Spark table name for training data.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--target-col\",\n",
    "        required=False,\n",
    "        default=get_env_or_default(\"TARGET_COL\", DEFAULT_TARGET_COL),\n",
    "        help=\"Target column name.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--registered-model-name\",\n",
    "        required=False,\n",
    "        default=get_env_or_default(\n",
    "            \"REGISTERED_MODEL_NAME\", DEFAULT_REGISTERED_MODEL_NAME\n",
    "        ),\n",
    "        help=\"Registered model name in MLflow Model Registry (Unity Catalog).\",\n",
    "    )\n",
    "\n",
    "    # Penting: supaya tidak error di notebook karena argumen -f\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# MLflow wrapper\n",
    "# ==============================\n",
    "class ProbaWrapper(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    Wrapper supaya predict() mengembalikan probability (kolom 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sk_model: Pipeline):\n",
    "        self.sk_model = sk_model\n",
    "\n",
    "    def predict(self, context: Any, model_input: pd.DataFrame) -> np.ndarray:\n",
    "        proba = self.sk_model.predict_proba(model_input)[:, 1]\n",
    "        return proba\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Load data\n",
    "# ==============================\n",
    "def load_data(table_name: str) -> pd.DataFrame:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    logger.info(f\"Loading data from Spark table: {table_name}\")\n",
    "    df_spark = spark.table(table_name)\n",
    "    df = df_spark.toPandas()\n",
    "    logger.info(f\"Loaded {len(df)} rows, {len(df.columns)} columns.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Build pipelines\n",
    "# ==============================\n",
    "def build_pipelines(X_train: pd.DataFrame):\n",
    "    numeric_cols = X_train.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "    categorical_cols = X_train.select_dtypes(\n",
    "        include=[\"object\", \"category\"]\n",
    "    ).columns.tolist()\n",
    "\n",
    "    logger.info(f\"Numeric columns    : {numeric_cols}\")\n",
    "    logger.info(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"0\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    logreg_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\n",
    "                \"clf\",\n",
    "                LogisticRegression(\n",
    "                    max_iter=800,\n",
    "                    class_weight=\"balanced\",\n",
    "                    n_jobs=-1,\n",
    "                    solver=\"lbfgs\",\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tree_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\n",
    "                \"clf\",\n",
    "                DecisionTreeClassifier(\n",
    "                    max_depth=5,\n",
    "                    min_samples_leaf=50,\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return logreg_pipeline, tree_pipeline\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Train + pilih best\n",
    "# ==============================\n",
    "def train_and_select_model(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    y_test: pd.Series,\n",
    "):\n",
    "    logreg_pipeline, tree_pipeline = build_pipelines(X_train)\n",
    "\n",
    "    logger.info(\"Training Logistic Regression...\")\n",
    "    logreg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    logger.info(\"Training Decision Tree...\")\n",
    "    tree_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_proba_log = logreg_pipeline.predict_proba(X_test)[:, 1]\n",
    "    y_proba_tree = tree_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    metrics: Dict[str, Dict[str, float]] = {\n",
    "        \"logistic\": {\n",
    "            \"roc_auc\": roc_auc_score(y_test, y_proba_log),\n",
    "            \"pr_auc\": average_precision_score(y_test, y_proba_log),\n",
    "        },\n",
    "        \"tree\": {\n",
    "            \"roc_auc\": roc_auc_score(y_test, y_proba_tree),\n",
    "            \"pr_auc\": average_precision_score(y_test, y_proba_tree),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    logger.info(\"Test metrics:\")\n",
    "    for name, m in metrics.items():\n",
    "        logger.info(\n",
    "            f\"  {name}: ROC-AUC={m['roc_auc']:.4f}, PR-AUC={m['pr_auc']:.4f}\"\n",
    "        )\n",
    "\n",
    "    best_name = max(metrics, key=lambda m: metrics[m][\"pr_auc\"])\n",
    "    best_model = logreg_pipeline if best_name == \"logistic\" else tree_pipeline\n",
    "\n",
    "    logger.info(f\"Best model selected (by PR-AUC): {best_name}\")\n",
    "\n",
    "    return best_model, best_name, metrics[best_name]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Log & register ke MLflow (minimal)\n",
    "# ==============================\n",
    "def log_and_register_model(\n",
    "    best_model: Pipeline,\n",
    "    best_model_name: str,\n",
    "    metrics: Dict[str, float],\n",
    "    registered_model_name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "):\n",
    "    \"\"\"\n",
    "    Log & register model ke MLflow, minimal:\n",
    "    - set_registry_uri(\"databricks-uc\")\n",
    "    - start_run()\n",
    "    - pyfunc.log_model(..., registered_model_name=...)\n",
    "    \"\"\"\n",
    "    # Sama seperti snippet yang sudah terbukti jalan:\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "    wrapped_model = ProbaWrapper(best_model)\n",
    "    y_train_proba = best_model.predict_proba(X_train)[:, 1]\n",
    "    signature = infer_signature(X_train, y_train_proba)\n",
    "    input_example = X_train.head(5)\n",
    "\n",
    "    logger.info(f\"Registering model to MLflow as: {registered_model_name}\")\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # Opsional: log sedikit info, tapi jangan yang ribet-ribet\n",
    "        mlflow.set_tag(\"model_family\", best_model_name)\n",
    "        mlflow.log_metric(\"test_roc_auc\", metrics[\"roc_auc\"])\n",
    "        mlflow.log_metric(\"test_pr_auc\", metrics[\"pr_auc\"])\n",
    "\n",
    "        mlflow.pyfunc.log_model(\n",
    "            python_model=wrapped_model,\n",
    "            artifact_path=\"bebas_aksi_proba_model\",\n",
    "            signature=signature,\n",
    "            input_example=input_example,\n",
    "            registered_model_name=registered_model_name,\n",
    "        )\n",
    "\n",
    "    logger.info(\"Model logged & registered successfully.\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Main\n",
    "# ==============================\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    logger.info(\"===== Training config =====\")\n",
    "    logger.info(f\"Feature table         : {args.feature_table}\")\n",
    "    logger.info(f\"Target column         : {args.target_col}\")\n",
    "    logger.info(f\"Registered model name : {args.registered_model_name}\")\n",
    "\n",
    "    df = load_data(args.feature_table)\n",
    "\n",
    "    if args.target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{args.target_col}' not found in data!\")\n",
    "\n",
    "    X = df.drop(columns=[args.target_col])\n",
    "    y = df[args.target_col]\n",
    "\n",
    "    counts = y.value_counts()\n",
    "    pct = y.value_counts(normalize=True) * 100\n",
    "    logger.info(\"Class distribution:\")\n",
    "    logger.info(\"\\n%s\", pd.DataFrame({\"count\": counts, \"pct\": pct.round(4)}))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    best_model, best_name, best_metrics = train_and_select_model(\n",
    "        X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    log_and_register_model(\n",
    "        best_model=best_model,\n",
    "        best_model_name=best_name,\n",
    "        metrics=best_metrics,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        X_train=X_train,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Training job completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caafc51e-ff4f-42de-8852-b209d986998a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8297222856366375,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Train_Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
